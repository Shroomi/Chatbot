{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This file is used to discuss the importance of\n",
    "    weight initialization.\n",
    "    \n",
    "    There are several ways of weight initialization.\n",
    "    \n",
    "    1. Pre-training\n",
    "    2. Random Initialization\n",
    "    3. Xavier Initialization\n",
    "    4. He Initialization\n",
    "    5. Batch Normalization Layer\n",
    "'''\n",
    "\n",
    "# Pre-training: Use the model which has been trained on task A\n",
    "#               for task B with fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input mean 0.00092 and std 1.00075\n",
      "layer 1: mean -0.00029 and std 0.26390\n",
      "layer 2: mean -0.00007 and std 0.07204\n",
      "layer 3: mean -0.00000 and std 0.01906\n",
      "layer 4: mean 0.00001 and std 0.00485\n",
      "layer 5: mean -0.00000 and std 0.00119\n",
      "layer 6: mean 0.00000 and std 0.00028\n",
      "layer 7: mean -0.00000 and std 0.00006\n",
      "layer 8: mean -0.00000 and std 0.00001\n",
      "layer 9: mean 0.00000 and std 0.00000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1820538d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random Initialization:\n",
    "# In the figure below, we can see the output gets lower and lower,\n",
    "# which means the gradient gets lower and lower.\n",
    "# It's difficult for us to update weights.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    data = tf.constant(np.random.randn(2000, 800).astype('float32'))\n",
    "    layer_sizes = [800 - 50 * i for i in range(0,10)]\n",
    "    num_layers = len(layer_sizes)\n",
    "    \n",
    "    fcs = [] # store the output of each layer\n",
    "    for i in range(0, num_layers - 1):\n",
    "        X = data if i == 0 else fcs[i - 1]\n",
    "        node_input = layer_sizes[i]\n",
    "        node_output = layer_sizes[i + 1]\n",
    "        W = tf.Variable(np.random.randn(node_input, node_output).astype\n",
    "                ('float32')) * 0.01 # Random Gaussian/Normal Distribution Initialization for weights\n",
    "        fc = tf.matmul(X, W)\n",
    "        fc = tf.nn.tanh(fc)\n",
    "        fcs.append(fc)\n",
    "        \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('input mean {0:.5f} and std {1:.5f}'.format(np.mean(data.eval()),\n",
    "                                                      np.std(data.eval())))\n",
    "    \n",
    "    for idx, fc in enumerate(fcs):\n",
    "        print('layer {0}: mean {1:.5f} and std {2:.5f}'.format(idx+1, np.mean(fc.eval()),\n",
    "                                                              np.std(fc.eval())))\n",
    "        \n",
    "    plt.figure()\n",
    "    for idx, fc in enumerate(fcs):\n",
    "        plt.subplot(1, len(fcs), idx+1)\n",
    "        plt.hist(fc.eval().flatten(), 30, range=[-1,1])\n",
    "        plt.xlabel('layer ' + str(idx + 1))\n",
    "        plt.yticks([])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input mean 0.00074 and std 1.00061\n",
      "layer 1: mean 0.00043 and std 0.98575\n",
      "layer 2: mean -0.00012 and std 0.98512\n",
      "layer 3: mean 0.00125 and std 0.98461\n",
      "layer 4: mean -0.00129 and std 0.98394\n",
      "layer 5: mean 0.00077 and std 0.98328\n",
      "layer 6: mean 0.00038 and std 0.98262\n",
      "layer 7: mean -0.00003 and std 0.98167\n",
      "layer 8: mean -0.00104 and std 0.98061\n",
      "layer 9: mean -0.00115 and std 0.97959\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18206b9a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random Initialization: Increase initialized weights\n",
    "# The result is in the figure below,\n",
    "# almost outputs are stay in -1 or 1,\n",
    "# and their gradients are close to 0.\n",
    "# It's also difficult for us to update weights.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    data = tf.constant(np.random.randn(2000, 800).astype('float32'))\n",
    "    layer_sizes = [800 - 50 * i for i in range(0,10)]\n",
    "    num_layers = len(layer_sizes)\n",
    "    \n",
    "    fcs = [] # store the output of each layer\n",
    "    for i in range(0, num_layers - 1):\n",
    "        X = data if i == 0 else fcs[i - 1]\n",
    "        node_input = layer_sizes[i]\n",
    "        node_output = layer_sizes[i + 1]\n",
    "        W = tf.Variable(np.random.randn(node_input, node_output).astype\n",
    "                ('float32')) # Random Gaussian/Normal Distribution Initialization for weights\n",
    "        fc = tf.matmul(X, W)\n",
    "        fc = tf.nn.tanh(fc)\n",
    "        fcs.append(fc)\n",
    "        \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('input mean {0:.5f} and std {1:.5f}'.format(np.mean(data.eval()),\n",
    "                                                      np.std(data.eval())))\n",
    "    \n",
    "    for idx, fc in enumerate(fcs):\n",
    "        print('layer {0}: mean {1:.5f} and std {2:.5f}'.format(idx+1, np.mean(fc.eval()),\n",
    "                                                              np.std(fc.eval())))\n",
    "        \n",
    "    plt.figure()\n",
    "    for idx, fc in enumerate(fcs):\n",
    "        plt.subplot(1, len(fcs), idx+1)\n",
    "        plt.hist(fc.eval().flatten(), 30, range=[-1,1])\n",
    "        plt.xlabel('layer ' + str(idx + 1))\n",
    "        plt.yticks([])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input mean 0.00028 and std 1.00102\n",
      "layer 1: mean -0.00015 and std 0.62808\n",
      "layer 2: mean 0.00053 and std 0.48619\n",
      "layer 3: mean 0.00015 and std 0.40780\n",
      "layer 4: mean 0.00007 and std 0.35766\n",
      "layer 5: mean 0.00062 and std 0.32055\n",
      "layer 6: mean -0.00035 and std 0.29434\n",
      "layer 7: mean 0.00065 and std 0.27109\n",
      "layer 8: mean 0.00026 and std 0.25368\n",
      "layer 9: mean -0.00018 and std 0.23876\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1824548e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Xavier Initialization：\n",
    "# The central idea is to keep the variances of output and input are the same\n",
    "# in order to avoid that all of outputs are close to 0.\n",
    "# But the inference of this initialization is based on linear function,\n",
    "# so it's not suitable for all of non-linear function.\n",
    "# The result is better(outputs keep appropriate distribution)in the figure because of 'tahn' function,\n",
    "# it just indicates that it's suitable for 'tanh' but not for all of non-linear function.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    data = tf.constant(np.random.randn(2000, 800).astype('float32'))\n",
    "    layer_sizes = [800 - 50 * i for i in range(0,10)]\n",
    "    num_layers = len(layer_sizes)\n",
    "    \n",
    "    fcs = [] # store the output of each layer\n",
    "    for i in range(0, num_layers - 1):\n",
    "        X = data if i == 0 else fcs[i - 1]\n",
    "        node_input = layer_sizes[i]\n",
    "        node_output = layer_sizes[i + 1]\n",
    "        W = (tf.Variable(np.random.randn(node_input, node_output).astype('float32')) \n",
    "             / np.sqrt(node_input)) # Xavier Initalization\n",
    "        fc = tf.matmul(X, W)\n",
    "        fc = tf.nn.tanh(fc)\n",
    "        fcs.append(fc)\n",
    "        \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('input mean {0:.5f} and std {1:.5f}'.format(np.mean(data.eval()),\n",
    "                                                      np.std(data.eval())))\n",
    "    \n",
    "    for idx, fc in enumerate(fcs):\n",
    "        print('layer {0}: mean {1:.5f} and std {2:.5f}'.format(idx+1, np.mean(fc.eval()),\n",
    "                                                              np.std(fc.eval())))\n",
    "        \n",
    "    plt.figure()\n",
    "    for idx, fc in enumerate(fcs):\n",
    "        plt.subplot(1, len(fcs), idx+1)\n",
    "        plt.hist(fc.eval().flatten(), 30, range=[-1,1])\n",
    "        plt.xlabel('layer ' + str(idx + 1))\n",
    "        plt.yticks([])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input mean -0.00012 and std 1.00120\n",
      "layer 1: mean 0.39984 and std 0.58485\n",
      "layer 2: mean 0.28709 and std 0.41486\n",
      "layer 3: mean 0.19198 and std 0.29232\n",
      "layer 4: mean 0.13950 and std 0.20493\n",
      "layer 5: mean 0.09120 and std 0.14017\n",
      "layer 6: mean 0.05732 and std 0.09071\n",
      "layer 7: mean 0.03722 and std 0.05802\n",
      "layer 8: mean 0.02462 and std 0.03745\n",
      "layer 9: mean 0.01892 and std 0.02660\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1820c0b5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Xavier Initialization:\n",
    "# change 'tahn' function into 'relu' function(They are both non-linear function).\n",
    "# The result shown in the figure is not good.\n",
    "# We can use 'He Initialization' to solve this problem.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    data = tf.constant(np.random.randn(2000, 800).astype('float32'))\n",
    "    layer_sizes = [800 - 50 * i for i in range(0,10)]\n",
    "    num_layers = len(layer_sizes)\n",
    "    \n",
    "    fcs = [] # store the output of each layer\n",
    "    for i in range(0, num_layers - 1):\n",
    "        X = data if i == 0 else fcs[i - 1]\n",
    "        node_input = layer_sizes[i]\n",
    "        node_output = layer_sizes[i + 1]\n",
    "        W = (tf.Variable(np.random.randn(node_input, node_output).astype('float32')) \n",
    "             / np.sqrt(node_input)) # Xavier Initalization\n",
    "        fc = tf.matmul(X, W)\n",
    "        fc = tf.nn.relu(fc)\n",
    "        fcs.append(fc)\n",
    "        \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('input mean {0:.5f} and std {1:.5f}'.format(np.mean(data.eval()),\n",
    "                                                      np.std(data.eval())))\n",
    "    \n",
    "    for idx, fc in enumerate(fcs):\n",
    "        print('layer {0}: mean {1:.5f} and std {2:.5f}'.format(idx+1, np.mean(fc.eval()),\n",
    "                                                              np.std(fc.eval())))\n",
    "        \n",
    "    plt.figure()\n",
    "    for idx, fc in enumerate(fcs):\n",
    "        plt.subplot(1, len(fcs), idx+1)\n",
    "        plt.hist(fc.eval().flatten(), 30, range=[-1,1])\n",
    "        plt.xlabel('layer ' + str(idx + 1))\n",
    "        plt.yticks([])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input mean 0.00072 and std 0.99997\n",
      "layer 1: mean 0.56402 and std 0.82664\n",
      "layer 2: mean 0.57753 and std 0.84292\n",
      "layer 3: mean 0.57037 and std 0.83684\n",
      "layer 4: mean 0.52870 and std 0.80534\n",
      "layer 5: mean 0.53431 and std 0.79726\n",
      "layer 6: mean 0.49011 and std 0.75222\n",
      "layer 7: mean 0.51924 and std 0.76296\n",
      "layer 8: mean 0.48861 and std 0.78873\n",
      "layer 9: mean 0.51722 and std 0.73781\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181f96ebe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# He Initialization:\n",
    "# The main idea is half of neurals are activated, the other half are 0.\n",
    "# The result is nice.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    data = tf.constant(np.random.randn(2000, 800).astype('float32'))\n",
    "    layer_sizes = [800 - 50 * i for i in range(0,10)]\n",
    "    num_layers = len(layer_sizes)\n",
    "    \n",
    "    fcs = [] # store the output of each layer\n",
    "    for i in range(0, num_layers - 1):\n",
    "        X = data if i == 0 else fcs[i - 1]\n",
    "        node_input = layer_sizes[i]\n",
    "        node_output = layer_sizes[i + 1]\n",
    "        W = (tf.Variable(np.random.randn(node_input, node_output).astype('float32')) \n",
    "             / np.sqrt(node_input/2)) # Xavier Initalization\n",
    "        fc = tf.matmul(X, W)\n",
    "        fc = tf.nn.relu(fc)\n",
    "        fcs.append(fc)\n",
    "        \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('input mean {0:.5f} and std {1:.5f}'.format(np.mean(data.eval()),\n",
    "                                                      np.std(data.eval())))\n",
    "    \n",
    "    for idx, fc in enumerate(fcs):\n",
    "        print('layer {0}: mean {1:.5f} and std {2:.5f}'.format(idx+1, np.mean(fc.eval()),\n",
    "                                                              np.std(fc.eval())))\n",
    "        \n",
    "    plt.figure()\n",
    "    for idx, fc in enumerate(fcs):\n",
    "        plt.subplot(1, len(fcs), idx+1)\n",
    "        plt.hist(fc.eval().flatten(), 30, range=[-1,1])\n",
    "        plt.xlabel('layer ' + str(idx + 1))\n",
    "        plt.yticks([])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-44-f35438fc0532>\", line 26, in <module>\n",
      "    fc = tf.contrib.layers.batch_norm(fc, center=True, scale=True)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\n",
      "    module = importlib.import_module(self.__name__)\n",
      "  File \"/anaconda3/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 36, in <module>\n",
      "    from tensorflow.contrib import distribute\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/__init__.py\", line 22, in <module>\n",
      "    from tensorflow.contrib.distribute.python.cross_tower_ops import *\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 24, in <module>\n",
      "    from tensorflow.contrib.distribute.python import values as value_lib\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py\", line 30, in <module>\n",
      "    from tensorflow.contrib.distribute.python import prefetching_ops_v2\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/prefetching_ops_v2.py\", line 24, in <module>\n",
      "    from tensorflow.contrib.data.python.ops import prefetching_ops\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/prefetching_ops.py\", line 175, in <module>\n",
      "    class _PrefetchToDeviceEagerIterator(iterator_ops.EagerIterator):\n",
      "AttributeError: module 'tensorflow.python.data.ops.iterator_ops' has no attribute 'EagerIterator'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/anaconda3/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/anaconda3/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/anaconda3/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/anaconda3/lib/python3.6/inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/anaconda3/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/anaconda3/lib/python3.6/posixpath.py\", line 374, in abspath\n",
      "    cwd = os.getcwd()\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.data.ops.iterator_ops' has no attribute 'EagerIterator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Batch Normalization Layer:\n",
    "# The basic idea: If you want it, just make it!\n",
    "# We want better distribution(e.g.Gaussian Distribution) before non-linear activation \n",
    "# in order to be easier to compute gradient and update weights.\n",
    "# Batch Normalization is used to force outputs to do Gaussian Normalization and Linear Transform.\n",
    "# The result is very nice! Recommand to use Batch Normalization.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    data = tf.constant(np.random.randn(2000, 800).astype('float32'))\n",
    "    layer_sizes = [800 - 50 * i for i in range(0,10)]\n",
    "    num_layers = len(layer_sizes)\n",
    "    \n",
    "    fcs = [] # store the output of each layer\n",
    "    for i in range(0, num_layers - 1):\n",
    "        X = data if i == 0 else fcs[i - 1]\n",
    "        node_input = layer_sizes[i]\n",
    "        node_output = layer_sizes[i + 1]\n",
    "        W = tf.Variable(np.random.randn(node_input, node_output).astype\n",
    "                ('float32')) * 0.01 # Random Gaussian/Normal Distribution Initialization for weights\n",
    "        fc = tf.matmul(X, W)\n",
    "        #%% The following code can't work in jupyter, but can work on my own mac.\n",
    "        fc = tf.contrib.layers.batch_norm(fc, center=True, scale=True)\n",
    "        fc = tf.nn.relu(fc)\n",
    "        fcs.append(fc)\n",
    "        \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('input mean {0:.5f} and std {1:.5f}'.format(np.mean(data.eval()),\n",
    "                                                      np.std(data.eval())))\n",
    "    \n",
    "    for idx, fc in enumerate(fcs):\n",
    "        print('layer {0}: mean {1:.5f} and std {2:.5f}'.format(idx+1, np.mean(fc.eval()),\n",
    "                                                              np.std(fc.eval())))\n",
    "        \n",
    "    plt.figure()\n",
    "    for idx, fc in enumerate(fcs):\n",
    "        plt.subplot(1, len(fcs), idx+1)\n",
    "        plt.hist(fc.eval().flatten(), 30, range=[-1,1])\n",
    "        plt.xlabel('layer ' + str(idx + 1))\n",
    "        plt.yticks([])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   Reference and Notes:\n",
    "   \n",
    "   glorot_normal: Glorot正态分布初始化方法，也称作Xavier正态分布初始化。\n",
    "   glorot_uniform: Glorot均匀分布初始化方法，又成Xavier均匀初始化。\n",
    "   \n",
    "   Xavier initialization是由Xavier Glorot et al.在2010年提出，\n",
    "   He initialization是由Kaiming He et al.在2015年提出，\n",
    "   Batch Normalization是由Sergey Ioffe et al.在2015年提出。\n",
    "   \n",
    "   1. https://keras-cn.readthedocs.io/en/latest/other/initializations/\n",
    "   2. https://zhuanlan.zhihu.com/p/25110150\n",
    "   3. Xavier Glorot et al., Understanding the Difficult of Training Deep Feedforward Neural Networks\n",
    "   4. Kaiming He et al., Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classfication\n",
    "   5. Sergey Ioffe et al., Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
    "   6. Standord CS231n\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
